<?xml version="1.0" encoding="UTF-8"?>
<!-- 日志级别从低到高分为TRACE < DEBUG < INFO < WARN < ERROR < FATAL，如果设置为WARN，则低于WARN的信息都不会输出 -->
<!-- scan:当此属性设置为true时，配置文档如果发生改变，将会被重新加载，默认值为true -->
<!-- scanPeriod:设置监测配置文档是否有修改的时间间隔，如果没有给出时间单位，默认单位是毫秒。
                 当scan为true时，此属性生效。默认的时间间隔为1分钟。 -->
<!-- debug:当此属性设置为true时，将打印出logback内部日志信息，实时查看logback运行状态。默认值为false。 -->
<configuration scan="true" scanPeriod="60 seconds">

	<contextName>logback</contextName>

	<include resource="org/springframework/boot/logging/logback/defaults.xml"/>

	<!-- name的值是变量的名称，value的值时变量定义的值。通过定义的值会被插入到logger上下文中。定义后，可以使“${}”来使用变量。 -->
	<property name="LOG_PATH" value="/data/im/log"/>
	<property name="LOG_NAME" value="im"/>

	<!--0. 日志格式和颜色渲染 -->
	<!-- 彩色日志依赖的渲染类 -->
	<conversionRule conversionWord="clr"
		converterClass="org.springframework.boot.logging.logback.ColorConverter"/>
	<conversionRule conversionWord="wex"
		converterClass="org.springframework.boot.logging.logback.WhitespaceThrowableProxyConverter"/>
	<conversionRule conversionWord="wEx"
		converterClass="org.springframework.boot.logging.logback.ExtendedWhitespaceThrowableProxyConverter"/>

	<!-- 彩色日志格式 -->
	<property name="CONSOLE_LOG_PATTERN"
		value="%X{serverIp}-%green([%X{traceId}]) ${CONSOLE_LOG_PATTERN:-%clr(%d{yyyy-MM-dd HH:mm:ss.SSS}){faint} %clr(${LOG_LEVEL_PATTERN:-%5p}) %clr(${PID:- }){magenta} %clr(---){faint} %clr([%15.15t]){faint} %clr(%-40.40logger{39}){cyan} %clr(:){faint} %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}}"/>

	<property name="FILE_LOG_PATTERN"
		value="%X{serverIp}-[%X{traceId}] ${FILE_LOG_PATTERN:-%d{yyyy-MM-dd HH:mm:ss.SSS} ${LOG_LEVEL_PATTERN:-%5p} ${PID:- } --- [%t] %-40.40logger{39} : %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}}"/>

	<!-- 格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度%msg：日志消息，%n是换行符 -->
	<property name="KAFKA_LOG_PATTERN"
		value="%X{serverIp}-[%X{traceId}] ${KAFKA_LOG_PATTERN:-%d{yyyy-MM-dd HH:mm:ss.SSS} ${LOG_LEVEL_PATTERN:-%5p} : %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}}"/>

	<!-- kafka 配置 -->
	<springProperty scope="context" name="kafka.servers" source="spring.kafka.producer.bootstrap-servers"/>
	<springProperty scope="context" name="kafka.topic" source="spring.kafka.template.default-topic"/>

	<!--1. 输出到控制台-->
	<appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
		<filter class="ch.qos.logback.classic.filter.ThresholdFilter">
			<level>DEBUG</level>
		</filter>
		<encoder>
			<pattern>${CONSOLE_LOG_PATTERN}</pattern>
			<!-- 设置字符集 -->
			<charset>UTF-8</charset>
		</encoder>
	</appender>

	<!--2. 输出到文档-->
	<!-- 2.1 level为 INFO 日志，时间滚动输出  -->
	<appender name="INFO" class="ch.qos.logback.core.rolling.RollingFileAppender">
		<!-- 此日志文档只记录info级别的 -->
<!--		<filter class="ch.qos.logback.classic.filter.LevelFilter">-->
<!--			&lt;!&ndash;过滤 INFO&ndash;&gt;-->
<!--			<level>INFO</level>-->
<!--			&lt;!&ndash;匹配到就允许&ndash;&gt;-->
<!--			<onMatch>ACCEPT</onMatch>-->
<!--			&lt;!&ndash;没有匹配到就禁止&ndash;&gt;-->
<!--			<onMismatch>DENY</onMismatch>-->
<!--		</filter>-->
		<!--日志文档输出格式-->
		<encoder>
			<!-- 此处设置字符集 -->
			<charset>UTF-8</charset>
			<pattern>${FILE_LOG_PATTERN}</pattern>
		</encoder>
		<!-- 日志记录器的滚动策略，按日期，按大小记录 -->
		<rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
			<!-- 配置生成路径 -->
			<fileNamePattern>${LOG_PATH}/${LOG_NAME}-info-%d{yyyy-MM-dd HH}.log</fileNamePattern>
			<!--日志文档保留天数-->
			<maxHistory>30</maxHistory>
			<!--用来指定日志文件的上限大小，那么到了这个值，就会删除旧的日志-->
			<!--<totalSizeCap>1GB</totalSizeCap>-->
		</rollingPolicy>
	</appender>

	<!-- 2.2 level为 WARN 日志，时间滚动输出  -->
	<appender name="WARN" class="ch.qos.logback.core.rolling.RollingFileAppender">
		<!-- 此日志文档只记录warn级别的 -->
		<filter class="ch.qos.logback.classic.filter.LevelFilter">
			<!--过滤 WARN-->
			<level>WARN</level>
			<!--匹配到就允许-->
			<onMatch>ACCEPT</onMatch>
			<!--没有匹配到就禁止-->
			<onMismatch>DENY</onMismatch>
		</filter>
		<!--日志文档输出格式-->
		<encoder>
			<!-- 此处设置字符集 -->
			<charset>UTF-8</charset>
			<pattern>${FILE_LOG_PATTERN}</pattern>
		</encoder>
		<!-- 日志记录器的滚动策略，按日期，按大小记录 -->
		<rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
			<!-- 配置生成路径 -->
			<fileNamePattern>${LOG_PATH}/${LOG_NAME}-warn-%d{yyyy-MM-dd HH}.log</fileNamePattern>
			<!--日志文档保留天数-->
			<maxHistory>30</maxHistory>
			<!--用来指定日志文件的上限大小，那么到了这个值，就会删除旧的日志-->
			<!--<totalSizeCap>1GB</totalSizeCap>-->
		</rollingPolicy>
	</appender>

	<!-- 2.3 level为 ERROR 日志，时间滚动输出  -->
	<appender name="ERROR" class="ch.qos.logback.core.rolling.RollingFileAppender">
		<!-- 此日志文档只记录ERROR级别的 -->
		<filter class="ch.qos.logback.classic.filter.LevelFilter">
			<!--过滤 ERROR-->
			<level>ERROR</level>
			<!--匹配到就允许-->
			<onMatch>ACCEPT</onMatch>
			<!--没有匹配到就禁止-->
			<onMismatch>DENY</onMismatch>
		</filter>
		<!--日志文档输出格式-->
		<encoder>
			<!-- 此处设置字符集 -->
			<charset>UTF-8</charset>
			<pattern>${FILE_LOG_PATTERN}</pattern>
		</encoder>
		<!-- 日志记录器的滚动策略，按日期，按大小记录 -->
		<rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
			<!-- 配置生成路径 -->
			<fileNamePattern>${LOG_PATH}/${LOG_NAME}-error-%d{yyyy-MM-dd HH}.log</fileNamePattern>
			<!--日志文档保留天数-->
			<maxHistory>30</maxHistory>
			<!--用来指定日志文件的上限大小，那么到了这个值，就会删除旧的日志-->
			<!--<totalSizeCap>1GB</totalSizeCap>-->
		</rollingPolicy>
	</appender>

	<!-- 2.4 KafkaAppender level为 DEBUG 日志，时间滚动输出  -->
	<appender name="KAFKA" class="com.github.danielwegener.logback.kafka.KafkaAppender">
		<!--此日志appender是为开发使用，只配置最底级别，控制台输出的日志级别是大于或等于此级别的日志信息-->
		<filter class="ch.qos.logback.classic.filter.ThresholdFilter">
			<level>DEBUG</level>
		</filter>
		<!--日志文档输出格式-->
		<encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder">
			<!-- 设置字符集 -->
			<charset>UTF-8</charset>
			<pattern>${KAFKA_LOG_PATTERN}</pattern>
		</encoder>
		<!-- kafka topic -->
		<topic>${kafka.topic}</topic>
		<keyingStrategy
			class="com.github.danielwegener.logback.kafka.keying.NoKeyKeyingStrategy"/>
		<deliveryStrategy
			class="com.github.danielwegener.logback.kafka.delivery.AsynchronousDeliveryStrategy"/>
		<!-- kafka 地址 -->
		<producerConfig>bootstrap.servers=${kafka.servers}</producerConfig>
		<producerConfig>acks=0</producerConfig>
		<producerConfig>linger.ms=1000</producerConfig>
		<producerConfig>max.block.ms=0</producerConfig>
		<producerConfig>client.id=${HOSTNAME}-${CONTEXT_NAME}-relaxed</producerConfig>
		<!-- <producerConfig>retries=1</producerConfig>-->
		<!-- <producerConfig>batch-size=16384</producerConfig>-->
		<!-- <producerConfig>buffer-memory=33554432</producerConfig>-->
		<!-- <producerConfig>properties.max.request.size==2097152</producerConfig>-->
		<!-- 如果kafka不可用则输出到 appender -->
		<appender-ref ref="INFO"/>
		<appender-ref ref="ERROR"/>
		<appender-ref ref="WARN"/>
	</appender>

	<!-- 异步传递策略，建议选择异步，不然连接kafka失败，会阻挡服务启动 -->
	<appender name="ASYNC" class="ch.qos.logback.classic.AsyncAppender">
		<neverBlock>true</neverBlock>
		<includeCallerData>true</includeCallerData>
		<discardingThreshold>0</discardingThreshold>
		<queueSize>2048</queueSize>
		<appender-ref ref="KAFKA"/>
	</appender>

	<!--
         <logger>用来设置某一个包或者具体的某一个类的日志打印级别、
         以及指定<appender>。<logger>仅有一个name属性，
         一个可选的level和一个可选的addtivity属性。
         name:用来指定受此logger约束的某一个包或者具体的某一个类。
         level:用来设置打印级别，大小写无关：TRACE, DEBUG, INFO, WARN, ERROR, ALL 和 OFF，
               还有一个特俗值INHERITED或者同义词NULL，代表强制执行上级的级别。
               如果未设置此属性，那么当前logger将会继承上级的级别。
         addtivity:是否向上级logger传递打印信息。默认是true。
         <logger name="org.springframework.web" level="info"/>
         <logger name="org.springframework.scheduling.annotation.ScheduledAnnotationBeanPostProcessor" level="INFO"/>
     -->

	<!--
      使用mybatis的时候，sql语句是debug下才会打印，而这里我们只配置了info，所以想要查看sql语句的话，有以下两种操作：
      第一种把<root level="info">改成<root level="DEBUG">这样就会打印sql，不过这样日志那边会出现很多其他消息
      第二种就是单独给dao下目录配置debug模式，代码如下，这样配置sql语句会打印，其他还是正常info级别：
      【logging.level.org.mybatis=debug logging.level.dao=debug】
   -->

	<!--
      root节点是必选节点，用来指定最基础的日志输出级别，只有一个level属性
      level:用来设置打印级别，大小写无关：TRACE, DEBUG, INFO, WARN, ERROR, ALL 和 OFF，
      不能设置为INHERITED或者同义词NULL。默认是DEBUG
      可以包含零个或多个元素，标识这个appender将会添加到这个logger。
  -->

	<logger name="com.cp.im.mapper" level="DEBUG"/>

	<!-- 4. 最终的策略 -->
	<root level="INFO">
		<springProfile name="dev">
			<appender-ref ref="CONSOLE"/>
			<appender-ref ref="ERROR" />
			<appender-ref ref="WARN" />
			<appender-ref ref="INFO" />
		</springProfile>
		<springProfile name="test,pre">
			<appender-ref ref="CONSOLE"/>
			<appender-ref ref="ERROR" />
			<appender-ref ref="WARN" />
			<appender-ref ref="INFO" />
		</springProfile>
		<springProfile name="pro">
			<appender-ref ref="CONSOLE"/>
			<!-- kafka 日志 -->
			<appender-ref ref="ASYNC"/>
		</springProfile>
	</root>

</configuration>